
import glob
import logging
import os
import os.path as osp
from datetime import datetime
import torch
import torch.nn as nn
from train import val_step_new
from utils.LossFn import LossFn
from utils.utils_functions import remove_handler, get_device, solv_num_workers, floating_type, get_split
import yaml
import json
import shutil
import utils.nmr_dataset as nmr_dataset
from torch_geometric.loader import DataLoader
from Networks.CSTShift import sPhysNet, CSTShiftEmb, CSTShiftOut
from torch.optim.swa_utils import AveragedModel
import pandas as pd


def init_model_test(config_dict, state_dict):
    if config_dict.get('ext_atom_method') == 'emb':
        net = CSTShiftEmb(**config_dict)
    elif config_dict.get('ext_atom_method') == 'out':
        net = CSTShiftOut(**config_dict)
    else:
        net = sPhysNet(**config_dict)
    model = net.to(get_device()).type(floating_type)
    model = AveragedModel(model)
    if "n_averaged" not in state_dict.keys():
        model.module.load_state_dict(state_dict)
    else:
        model.load_state_dict(state_dict)
    return model


class TrainedFolder:
    """
    Load a trained folder for performance evaluation and other purposes.
    A trained folder is a folder generated by train.py. 
    """
    def __init__(self, folder_name=None, 
                 new_test_dir_prefix=None,
                 dataset_args=None,
                 dataset_class=None,
                 split=None,
                 ignore_train=True, 
                 ignore_val=False, 
                 labeled_data=False,
                 explicit_ds_config=None,
                 avg_iso_atoms=False):
        self.folder_name = folder_name
        self.new_test_dir_prefix = new_test_dir_prefix
        self.dataset_args = dataset_args
        self.dataset_class = dataset_class if dataset_class is not None else 'NMRDatasetFromProcessed'
        self.labeled_data = labeled_data
        self.split_file_path = split
        self.ignore_train = ignore_train
        self.ignore_val = ignore_val
        self.avg_iso_atoms = avg_iso_atoms
        self.args = self.get_args()
        self.loss_fn = LossFn(config_dict=self.args, **self.args)
        self.test_dir = self.get_test_dir()
        os.mkdir(self.test_dir)
        self.set_logger()
        self.num_workers = self.get_num_workers()
        self.explicit_ds_config = explicit_ds_config
        self.get_dataset()
        self.model = self.load_model()

    def get_dataset(self):
        if self.dataset_args is not None:
            dataset_class = getattr(nmr_dataset, self.dataset_class)
            self.data_provider = dataset_class(**self.dataset_args)
            # default split is using all the data for testing
            self.splits = get_split(self.split_file_path) if self.split_file_path else get_split(all_test=len(self.data_provider.data['N']))
        else:  # read from original dataset
            dataset_class = getattr(nmr_dataset, self.args['data']['dataset_class']) if 'dataset_class' in self.args['data'] else nmr_dataset.NMRDatasetFromProcessed
            self.data_provider = dataset_class(**self.args['data']['dataset_args'])
            self.splits = get_split(self.args['data']['split']) if 'split' in self.args['data'] else get_split(all_test=len(self.data_provider.data['N']))
    
    def get_test_dir(self):
        if self.new_test_dir_prefix:
            return self.new_test_dir_prefix + '_' + datetime.now().strftime('%m%d_%H%M%S')
        else:
            return osp.join(self.folder_name, osp.basename('test' + datetime.now().strftime('%m%d_%H%M%S')))

    def get_args(self):
        self.config_name = glob.glob(osp.join(self.folder_name, 'config.yaml'))[0]
        with open(self.config_name, 'r') as yaml_file:
            args = yaml.safe_load(yaml_file)
        return args
    
    def get_num_workers(self):
        n_cpu_avail, n_cpu, num_workers = solv_num_workers()
        self.logger.info(f"Number of total CPU: {n_cpu}")
        self.logger.info(f"Number of available CPU: {n_cpu_avail}")
        self.logger.info(f"Number of workers used: {num_workers}")
        return num_workers
    
    def set_logger(self):
        remove_handler()
        logging.basicConfig(filename=os.path.join(self.test_dir, "test.log"),
                            format='%(asctime)s %(message)s', filemode='w')
        self.logger = logging.getLogger()
        self.logger.setLevel(logging.DEBUG)
    
    def load_model(self):
        model_data = torch.load(os.path.join(self.folder_name, 'best_model.pt'), map_location=get_device())
        return init_model_test(self.args, model_data)

    def run_test(self):
        shutil.copy(self.config_name, self.test_dir)  # copy the config file to the test folder
        self.logger.info("dataset: {}".format(self.data_provider.processed_file_names))
        self.logger.info("dataset test: {}".format(self.data_provider.processed_file_names))
        for index_name in ["train_index", "val_index", "test_index"]:
            self.eval_ds(self.data_provider, self.splits, index_name)
        self.save_config()
        # remove global variables
        remove_handler()

    def eval_ds(self, this_ds, splits, index_name):
        if self.ignore_train and index_name == "train_index": return
        if self.ignore_val and index_name == "val_index": return

        self.logger.info(f"Testing on {index_name}")
        index_short = index_name.split("_")[0]
        result_name = f"loss_{index_short}.pt"
        result_file = osp.join(self.test_dir, result_name)
        if osp.exists(result_file):
            print(f"{osp.abspath(result_file)} exists, skipping...")
            self.logger.info(f"{osp.abspath(result_file)} exists, skipping...")
            return

        this_index = splits[index_name]
        if this_index is None:
            # for external test datasets where train_index and val_index are None
            return
        this_index = torch.as_tensor(this_index)

        self.logger.info("{} size: {}".format(index_name, len(this_index)))

        this_dl = DataLoader(this_ds[torch.as_tensor(this_index)], batch_size=self.args["valid_batch_size"],
                             shuffle=False, num_workers=self.num_workers)
        this_info = self.test_step(this_dl, result_file)
        self.logger.info("-------------- {} ---------------".format(index_short))
        for key in this_info:
            d = this_info[key]
            if isinstance(d, torch.Tensor):
                if torch.numel(d) == 1:
                    self.logger.info("{}: {}".format(key, this_info[key].item()))
                else:
                    self.logger.info("{} with shape: {}".format(key, this_info[key].shape))
            else:
                self.logger.info("{}: {}".format(key, this_info[key]))
        self.logger.info("----------- end of {} ------------".format(index_short))

    def test_step(self, data_loader, result_file):
        if self.labeled_data:
            result = val_step_new(self.model, data_loader, self.loss_fn, mol_lvl_detail=True, avg_iso_atoms=self.avg_iso_atoms)
            result["target_names"] = self.loss_fn.target_names
            torch.save(result, result_file)
            return result
        else:
            return self.test_step_unlabeled(data_loader, result_file)
    
    def test_step_unlabeled(self, data_loader, result_file):
        """test step for unlabeled data, the loss is not calculated. 
        Prediction along with atom index and molecule id are saved to a csv file.
        """
        self.model.eval()
        prop_pred = []
        atom_index_in_mols = []
        mol_id = []
        with torch.set_grad_enabled(False):
            for val_data in data_loader:
                val_data = val_data.to(get_device())
                this_prop_pred = self.model(val_data)["atom_prop"]
                this_atom_index = torch.cat([torch.arange(n) for n in val_data.N], dim=0)
                this_mol_id = []
                for i in range(len(val_data['mol_id'])):  # TODO
                    this_mol_id.extend([val_data['mol_id'][i]] * val_data.N[i])
                if self.args['mask_atom']:
                    mask = val_data.mask.bool()
                    this_prop_pred = this_prop_pred[mask, :]
                    this_atom_index = this_atom_index[mask]
                    this_mol_id = [this_mol_id[i] for i in range(len(this_mol_id)) if mask[i]]
                prop_pred.append(this_prop_pred)
                atom_index_in_mols.append(this_atom_index)
                mol_id.extend(this_mol_id)
        prop_pred = torch.cat(prop_pred, dim=0)
        atom_index_in_mols = torch.cat(atom_index_in_mols, dim=0)
        result_csv = result_file.replace('.pt', '.csv')
        result_df = pd.DataFrame(prop_pred.detach().cpu().numpy(), columns=['PROP_PRED'])
        result_df['ATOM_INDEX'] = atom_index_in_mols.detach().cpu().numpy()
        result_df['MOL_ID'] = mol_id
        result_df.to_csv(result_csv, index=False)
        return {"PROP_PRED": prop_pred}

    def save_config(self):
        chk_dict = {}
        vars_self = vars(self)
        for key in vars_self.keys():
            if isinstance(vars_self[key], (int, float, str, bool)):
                chk_dict[key] = vars_self[key]
        with open(osp.join(self.test_dir, "chk.json"), "w") as f:
            json.dump(chk_dict, f, indent=2)


class EnsembleTrainedModel(nn.Module):
    """
    Ensemble of trained models for test only. Not able to be trained.
    """
    def __init__(self, model_list, pooling='mean'):
        super().__init__()
        self.model_list = model_list
        self.pooling = pooling

    def forward(self, data):
        out_list = [model(data)['atom_prop'] for model in self.model_list]
        return {"atom_prop": torch.mean(torch.stack(out_list), dim=0)}


class EmsembleTrainedFolder(TrainedFolder):
    """
    Class to evaluate ensemble models. 
    """
    def __init__(self, 
                 folder_name_list,
                 **kwargs):
        self.folder_name_list = folder_name_list
        super().__init__(**kwargs)

    def get_args(self):
        # read all the config files in the folder_name_list
        args_list = []
        for folder_name in self.folder_name_list:
            config_name = glob.glob(osp.join(folder_name, 'config.yaml'))[0]
            with open(config_name, 'r') as yaml_file:
                args_list.append(yaml.safe_load(yaml_file))
        # all the args should be the same for ensemble models
        assert all(args == args_list[0] for args in args_list)  # TODO: discard some keys that are not necessary to be the same
        self.config_name = glob.glob(osp.join(self.folder_name_list[0], 'config.yaml'))[0]
        return args_list[0]

    def load_model(self):
        self.ensemble_model_list = []
        for folder_name in self.folder_name_list:
            model_data = torch.load(os.path.join(folder_name, 'best_model.pt'), map_location=get_device())
            model = init_model_test(self.args, model_data)
            self.ensemble_model_list.append(model)
        ens_model = EnsembleTrainedModel(self.ensemble_model_list)
        return ens_model

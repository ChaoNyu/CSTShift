import argparse
import glob
import logging
import os
import os.path as osp
from datetime import datetime
import torch
from train import val_step_new
from utils.LossFn import LossFn
from utils.utils_functions import remove_handler, get_device, solv_num_workers, floating_type, get_split
import yaml
import json
import shutil
import utils.nmr_dataset as nmr_dataset
from torch_geometric.loader import DataLoader
from src.Networks.CSTShift import sPhysNet, CSTShiftEmb, CSTShiftOut
from torch.optim.swa_utils import AveragedModel


def init_model_test(config_dict, state_dict):
    if config_dict.get('ext_atom_method') == 'emb':
        net = CSTShiftEmb(**config_dict)
    elif config_dict.get('ext_atom_method') == 'out':
        net = CSTShiftOut(**config_dict)
    else:
        net = sPhysNet(**config_dict)
    model = net.to(get_device()).type(floating_type)
    model = AveragedModel(model)
    if "n_averaged" not in state_dict.keys():
        model.module.load_state_dict(state_dict)
    else:
        model.load_state_dict(state_dict)
    return model


class TrainedFolder:
    """
    Load a trained folder for performance evaluation and other purposes.
    A trained folder is a folder generated by train.py. 
    """
    def __init__(self, folder_name=None, 
                 ignore_train=True, ignore_val=False, 
                 explicit_ds_config=None):
        self.folder_name = folder_name
        self.ignore_train = ignore_train
        self.ignore_val = ignore_val
        self.args = self.get_args()
        self.loss_fn = LossFn(config_dict=self.args, **self.args)
        self.test_dir = self.get_test_dir()
        os.mkdir(self.test_dir)
        self.set_logger()
        self.num_workers = self.get_num_workers()
        self.explicit_ds_config = explicit_ds_config
        self.get_dataset()
        self.model = self.load_model()

    def get_dataset(self):
        if self.explicit_ds_config is not None:
            with open(self.explicit_ds_config, 'r') as yaml_file:
                self.ds_args = yaml.safe_load(yaml_file)
            assert all(k in self.ds_args.keys() for k in ['dataset_class', 'split', 'dataset_args'])
            self.logger.info("Using explicit dataset config: {}".format(self.explicit_ds_config))
            dataset_class = getattr(nmr_dataset, self.args['data']['dataset_class']) if 'dataset_class' in self.args['data'] else nmr_dataset.NMRDatasetFromProcessed
            self.data_provider = dataset_class(**self.ds_args['dataset_args'])
            self.splits = get_split(self.ds_args['split'])
        else:  # read from original dataset
            dataset_class = getattr(nmr_dataset, self.args['data']['dataset_class']) if 'dataset_class' in self.args['data'] else nmr_dataset.NMRDatasetFromProcessed
            self.data_provider = dataset_class(**self.args['data']['dataset_args'])
            self.splits = get_split(self.args['data']['split'])
    
    def get_test_dir(self):
        return osp.join(self.folder_name, osp.basename('test' + datetime.now().strftime('%m%d_%H%M%S')))

    def get_args(self):
        self.config_name = glob.glob(osp.join(self.folder_name, 'config.yaml'))[0]
        with open(self.config_name, 'r') as yaml_file:
            args = yaml.safe_load(yaml_file)
        return args
    
    def get_num_workers(self):
        n_cpu_avail, n_cpu, num_workers = solv_num_workers()
        self.logger.info(f"Number of total CPU: {n_cpu}")
        self.logger.info(f"Number of available CPU: {n_cpu_avail}")
        self.logger.info(f"Number of workers used: {num_workers}")
        return num_workers
    
    def set_logger(self):
        remove_handler()
        logging.basicConfig(filename=os.path.join(self.test_dir, "test.log"),
                            format='%(asctime)s %(message)s', filemode='w')
        self.logger = logging.getLogger()
        self.logger.setLevel(logging.DEBUG)
    
    def load_model(self):
        model_data = torch.load(os.path.join(self.folder_name, 'best_model.pt'), map_location=get_device())
        return init_model_test(self.args, model_data)

    def run_test(self):
        shutil.copy(self.config_name, self.test_dir)  # copy the config file to the test folder
        self.logger.info("dataset: {}".format(self.data_provider.processed_file_names))
        self.logger.info("dataset test: {}".format(self.data_provider.processed_file_names))
        for index_name in ["train_index", "val_index", "test_index"]:
            self.eval_ds(self.data_provider, self.splits, index_name)
        self.save_config()
        # remove global variables
        remove_handler()

    def eval_ds(self, this_ds, splits, index_name):
        if self.ignore_train and index_name == "train_index": return
        if self.ignore_val and index_name == "val_index": return

        self.logger.info(f"Testing on {index_name}")
        index_short = index_name.split("_")[0]
        result_name = f"loss_{index_short}.pt"
        result_file = osp.join(self.test_dir, result_name)
        if osp.exists(result_file):
            print(f"{osp.abspath(result_file)} exists, skipping...")
            self.logger.info(f"{osp.abspath(result_file)} exists, skipping...")
            return

        this_index = splits[index_name]
        if this_index is None:
            # for external test datasets where train_index and val_index are None
            return
        this_index = torch.as_tensor(this_index)

        self.logger.info("{} size: {}".format(index_name, len(this_index)))

        this_dl = DataLoader(this_ds[torch.as_tensor(this_index)], batch_size=self.args["valid_batch_size"],
                             shuffle=False, num_workers=self.num_workers)
        this_info = self.test_step(this_dl, result_file)
        self.logger.info("-------------- {} ---------------".format(index_short))
        for key in this_info:
            d = this_info[key]
            if isinstance(d, torch.Tensor):
                if torch.numel(d) == 1:
                    self.logger.info("{}: {}".format(key, this_info[key].item()))
                else:
                    self.logger.info("{} with shape: {}".format(key, this_info[key].shape))
            else:
                self.logger.info("{}: {}".format(key, this_info[key]))
        self.logger.info("----------- end of {} ------------".format(index_short))

    def test_step(self, data_loader, result_file):
        result = val_step_new(self.model, data_loader, self.loss_fn, mol_lvl_detail=True)
        result["target_names"] = self.loss_fn.target_names
        torch.save(result, result_file)
        return result

    def save_config(self):
        chk_dict = {}
        vars_self = vars(self)
        for key in vars_self.keys():
            if isinstance(vars_self[key], (int, float, str, bool)):
                chk_dict[key] = vars_self[key]
        with open(osp.join(self.test_dir, "chk.json"), "w") as f:
            json.dump(chk_dict, f, indent=2)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--folder_name', type=str)
    parser.add_argument("--explicit_ds_config", default=None, type=str,
                        help="Explicitly specify dataset config, used when testing on an external dataset.")  
    # e.g. "--explicit_ds_config configs/test_cheshire.yaml" can direct the tester to use the config file. 
    # The config needed in explicit ds config files is in explicit_ds_args of TrainedFolder
    parser.add_argument('--ignore_train', action="store_true", default=True)
    parser.add_argument('--ignore_val', action="store_true")  # If ignore_val is True, the validation set will not be used. This is used for external datasets like cheshire dataset where all the data is used for testing
    _args = parser.parse_args()
    kwargs = vars(_args)

    print('testing folder: {}'.format(kwargs['folder_name']))
    tester = TrainedFolder(**kwargs)
    tester.run_test()
